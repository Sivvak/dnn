{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer\n",
    "In this lab scenario, you will implement causal attention for a Transformer Decoder model.\n",
    "Transformer architecture was introduced in the [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "and has dominated the field of language modeling.  \n",
    "Here we will go through different parts of the transformer architecture and explain each of them briefly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input\n",
    "Transformer decoder models (such as LLaMa 3.1 and Mistral) are popular text-processing models.   \n",
    "One can distinguish two versions of such models: base and instruction-tuned.  \n",
    "The base models are usually transformers trained on predicting the continuation of a given text (for each prefix they output a probability distribution over the next text fragments).  \n",
    "In contrast, the instruction-tuned ones are base models that were additionally trained to follow instructions.  \n",
    "The text is presented to the transformer as a sequence of tokens.   \n",
    "Tokens are integers used to represent pieces of text.  \n",
    "To be more precise to convert text to tokens we first prepare a dictionary of common text fragments.   \n",
    "We usually want to have all possible letters in the dictionary so that all texts can be tokenized.   \n",
    "We then assign to each text piece from the dictionary an integer and use the dictionary to convert text into a sequence of tokens (integers).  \n",
    "The program that converts text into tokens is called a tokenizer.  \n",
    "\n",
    "In this lab scenario, we will use OpenLLaMAv2 tokenizer and HuggingFace library to tokenize text.   \n",
    "HuggingFace contains a vast collection of transformer model weights and implementations along with training and inference code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers==4.47.0\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"openlm-research/open_llama_3b_v2\")\n",
    "\n",
    "\n",
    "text = \"This is an example text that we will tokenize\"\n",
    "tokens_mask = tokenizer(text)\n",
    "print(tokens_mask)\n",
    "\n",
    "detokenized = tokenizer.batch_decode(tokens_mask[\"input_ids\"])\n",
    "print(detokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization the the HuggingFace tokenizer returns a sequence of tokens (`input_ids`) and information on whether the model should look at the ith element of the input (`attention_mask`).  \n",
    "The other part is useful when we want to tokenize several sequences into one batch of elements of the same length. Then the attention mask can be used to hide the padding from the model.  \n",
    "Consider the example below. Note how the second text is padded to match the length of the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"This is an example text that we will tokenize\", \"Hello\"]\n",
    "# We set the padding token to be the same as the end-of-sequence token (eos)\n",
    "# eos token (</s> in this case) is used to mark the end of the sequence in training and can also be used by a model to finish its response\n",
    "# bos token (here <s>) is used to mark the beginning of the input\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokens_mask = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "print(tokens_mask)\n",
    "\n",
    "detokenized = tokenizer.batch_decode(tokens_mask[\"input_ids\"])\n",
    "print(detokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoding\n",
    "The input to the model is a batch of token sequences of the following shape   \n",
    "`(batch, seq_len)`\n",
    "where\n",
    "* `batch` is the size of the batch\n",
    "* `seq_len` is the length of the longest input sequence inside the batch (attention mask is used to handle the cases when sequences have different lengths)\n",
    "\n",
    "Initially, the model assigns to each element of each sequence an embedding vector.  \n",
    "To be more precise inside the model there is a matrix of shape `(num_dictionary_elements, hidden_size)` that is used to assign to each token from the dictionary a vector of length `hidden_size`.  \n",
    "After the encoding step, we pass a tensor of shape `(batch, seq_len, hidden_size)` through the remaining layers of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer layer\n",
    "The internal parts of the transformer are grouped into transformer layers.  \n",
    "Usually, each layer consists of layer norm, attention, layer norm, and a feed-forward layer.  \n",
    "To be more precise the computation progresses roughly as presented below:\n",
    "```python3\n",
    "def transformer_layer(input, layer_norm_attn, attention, layer_norm_ff, feed_forward):\n",
    " x = attention(layer_norm_attn(x)) + x\n",
    " x = feed_forward(layer_norm_ff(x)) + x\n",
    " return x\n",
    "```\n",
    "Where:  \n",
    "* `feed_forward` - This can be a linear projection followed by activation and another linear projection. For an input of shape `(batch, seq_len, hidden_size)` it treats the first two dimensions as batch and operates on the `hidden_size` dimension.\n",
    "* `layer_norm` - Replaced by [RMSNorm](https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html) in LLaMa models. Similarly as `feed_forward` it operates only on the `hidden_size` dimension, treating other dimensions as a batch.\n",
    "* `attention` - Causal multi-head attention that you will implement in further parts of this notebook. Let `t` be an input tensor of shape `(batch, seq_len, hidden_size)`. Attention will output a tensor `d` of the same shape with the following property:\n",
    " Calculation of `d[b, s, h]` depends only on values from `t[b, s', h']` such that  `s' <= s`. In other words, calculation is done independently per batch entry and dependency is causal (the past can influence the future but the future cannot influence the past)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM head\n",
    "In the end, a linear projection is used to create weights for each element of the input dictionary.\n",
    "To be more precise we take a tensor of shape `(batch, seq_len, hidden_dim)` and use norm + a linear projection from `hidden_dim` to `vocab_size`, in order to change it into tensor of shape `(batch, seq_len, vocab_size)`.  \n",
    "Then we apply softmax over the last dimension (`vocab_size`) to get probability distribution over the next token in the sequence given the previous tokens.  \n",
    "We can do this as all operations in our model were either done independently for each element (`layer_norm`, `feed_forward`, ...) or were causal (`attention`).  \n",
    "The training loss of our model will be cross entropy over the next token prediction.  \n",
    "That is we input a batch of token sequences into our model, the model outputs for each input token the probability distribution over the next token, and as we know the next token we use it as a ground truth label for the calculation of the model loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Below we show the steps described above using OpenLLaMAv2 3B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input tokenization\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"openlm-research/open_llama_3b_v2\")\n",
    "text = [\"2 + 7 = \"]\n",
    "\n",
    "tokens_mask = tokenizer(text, return_tensors=\"pt\")\n",
    "tokens = tokens_mask[\"input_ids\"]\n",
    "attention_mask = tokens_mask[\"attention_mask\"]\n",
    "print(tokens_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model from huggingface\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "# takes around 6.85GB in bf16\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"openlm-research/open_llama_3b_v2\", torch_dtype=torch.bfloat16, device_map=device\n",
    ")\n",
    "\n",
    "# we disable gradient calculatoin as otherwise the memory usage can explode\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input encoding\n",
    "embedded_tokens = model.model.embed_tokens(tokens.to(device))\n",
    "print(f\"tokens.shape {tokens.shape} embedded_tokens.shape {embedded_tokens.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## passing through the layers of the model\n",
    "\n",
    "hidden_states = embedded_tokens\n",
    "batch, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "# additional tensor to tell the model positions of each token\n",
    "position_ids = torch.arange(seq_len, device=hidden_states.device)[None, ...]\n",
    "\n",
    "# mask used to make attention causal\n",
    "causal_mask = model.model._update_causal_mask(\n",
    "    attention_mask, hidden_states, position_ids, None, False\n",
    ")\n",
    "\n",
    "# additional encoding of positions within the sequence, used by attention\n",
    "position_embeddings = model.model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "\n",
    "for l in tqdm(model.model.layers):\n",
    "    hidden_states = l(\n",
    "        hidden_states,\n",
    "        attention_mask=causal_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=None,  # can be used to continue generation\n",
    "        output_attentions=False,\n",
    "        use_cache=False,\n",
    "        cache_position=position_ids,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )[0]\n",
    "\n",
    "# apply norm before final linear\n",
    "hidden_states = model.model.norm(hidden_states)\n",
    "hidden_states = model.lm_head(hidden_states)\n",
    "hidden_states = torch.nn.functional.softmax(hidden_states, dim=-1)\n",
    "next_token = torch.argmax(hidden_states[0, -1])\n",
    "print(next_token)\n",
    "print(tokenizer.decode(next_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using HuggingFace Generate\n",
    "\n",
    "\n",
    "text = \"The largest animal on earth is\"\n",
    "tokens_mask = tokenizer(text, return_tensors=\"pt\")\n",
    "output = model.generate(\n",
    "    inputs=tokens_mask[\"input_ids\"].to(device),\n",
    "    max_new_tokens=8,\n",
    "    num_beams=1,\n",
    "    do_sample=True, # sample from the distribution created by softmax\n",
    "    temperature=0.7, # divide pre softmax score by this value\n",
    "    top_p=0.9 # cut out improbable tokens from sampling\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Attention Implementation\n",
    "\n",
    "Your task is to finish the implementation of the attention mechanism below. In case of problems, you can refer to the original implementation that can be found [here](https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/models/llama/modeling_llama.py#L258).\n",
    "To be more precise. You are given query and key tensors with positional encoding already applied. You also get the value tensors.\n",
    "Each of those tensors is of shape `(batch, seq_len, num_heads, head_size)`.  \n",
    "Your task is to compute for each head a scaled dot product between each query and each key that is either at the same position as the query or precedes the query in the sequence.\n",
    "To be more precise you want to calculate a tensor `a` of shape `(batch, num_heads, seq_len, seq_len)` where  \n",
    "$$\n",
    "    a[b, h, q, k]= \n",
    "\\begin{cases}\n",
    "    \\sum_{d}{\\mathrm{query}[b, q, h, d] * \\mathrm{key}[b, k, h, d]} / \\sqrt{\\mathrm{head\\_size}}, & if k \\leq q\\\\\n",
    "     -\\mathrm{large\\_number},              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then you should calculate the softmax over the last dimension of `a` creating `p`.  \n",
    "\n",
    "$$p = \\mathrm{SoftMax}(a)$$\n",
    "Then you should calculate \n",
    "$$v[b, q, h, d] = \\sum_{k}{a[b, h, q, k] * \\mathrm{value}[b, k, h, d]}$$  \n",
    "That is for each query you should gather the `value`s using the probability distribution defined by `p`.  \n",
    "In the end, you should reshape `v` to `(batch, seq_len, num_heads * head_size)` and apply a linear projection `output_projection`.  \n",
    "\n",
    "As you do not get the attention mask you can assume that it consists of ones only and that the attention is causal.\n",
    "For simplicity, you can also assume that the number of queries is equal to the number of keys.  \n",
    "This is not always true, for example when we run the generate from HuggingFace transformers library, then instead of computing the whole attention each time, the keys for previous tokens are cached and we create queries only for new tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_forward(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    output_projection: torch.nn.Linear,\n",
    ") -> torch.Tensor:\n",
    "    batch, q_seq_len, num_heads, head_dim = query.shape\n",
    "    batch, k_seq_len, num_heads, head_dim = key.shape\n",
    "\n",
    "    assert value.shape == key.shape\n",
    "\n",
    "    assert q_seq_len <= k_seq_len\n",
    "    assert query.shape[0] == key.shape[0]\n",
    "    assert query.shape[2:] == key.shape[2:]\n",
    "\n",
    "    # TODO {\n",
    "\n",
    "    # TODO }\n",
    "    assert v.shape == (batch, q_seq_len, num_heads * head_dim)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with OpenLLaMA\n",
    "The code below integrades your solution from above with OpenLLaMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "# Copied from  https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/models/llama/modeling_llama.py\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "# Copied from  https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/models/llama/modeling_llama.py\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(\n",
    "        batch, num_key_value_heads, n_rep, slen, head_dim\n",
    "    )\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "# modified version of https://github.com/huggingface/transformers/blob/7f95372c6267d3163fd2aa74aeff9d84ddb6cc35/src/transformers/models/llama/modeling_llama.py\n",
    "def custom_attention_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value=None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    position_embeddings: Optional[\n",
    "        Tuple[torch.Tensor, torch.Tensor]\n",
    "    ] = None,  # will become mandatory in v4.46\n",
    "    **kwargs,\n",
    "):\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = self.q_proj(hidden_states)\n",
    "    key_states = self.k_proj(hidden_states)\n",
    "    value_states = self.v_proj(hidden_states)\n",
    "\n",
    "    query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    if position_embeddings is None:\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "    else:\n",
    "        cos, sin = position_embeddings\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "        key_states, value_states = past_key_value.update(\n",
    "            key_states, value_states, self.layer_idx, cache_kwargs\n",
    "        )\n",
    "\n",
    "    key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "    # this is not memory optimal, can you tell why\n",
    "    query_states = query_states.transpose(1, 2)\n",
    "    key_states = key_states.transpose(1, 2)\n",
    "    value_states = value_states.transpose(1, 2)\n",
    "\n",
    "    attn_output = attention_forward(\n",
    "        query=query_states,\n",
    "        key=key_states,\n",
    "        value=value_states,\n",
    "        output_projection=self.o_proj,\n",
    "    )\n",
    "\n",
    "    return attn_output, None, past_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "You can briefly test your solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"openlm-research/open_llama_3b_v2\", torch_dtype=torch.bfloat16, device_map=device\n",
    ")\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for l in model.model.layers:\n",
    "    l.self_attn.forward = partial(custom_attention_forward, self=l.self_attn)\n",
    "\n",
    "\n",
    "text = [\"2 + 7 = \"]\n",
    "\n",
    "tokens_mask = tokenizer(text, return_tensors=\"pt\")\n",
    "tokens = tokens_mask[\"input_ids\"]\n",
    "attention_mask = tokens_mask[\"attention_mask\"]\n",
    "\n",
    "\n",
    "output = model(input_ids=tokens.to(device))\n",
    "next_token = torch.argmax(output.logits[0, -1])\n",
    "print(next_token)\n",
    "decoded = tokenizer.decode(next_token)\n",
    "print(f\"Model answer: {decoded}\")\n",
    "assert decoded == \"9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you have implemented the attention that can handle token by token generaion you can check your solution using the code below\n",
    "\n",
    "text = \"Solve x + 3 = 7\"\n",
    "tokens_mask = tokenizer(text, return_tensors=\"pt\")\n",
    "output = model.generate(\n",
    "    inputs=tokens_mask[\"input_ids\"].to(device),\n",
    "    max_new_tokens=8,\n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
