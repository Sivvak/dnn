{"cells":[{"cell_type":"markdown","metadata":{"id":"84VetyCaGLyR"},"source":["<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n","\n","AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n","<hr>\n","\n","<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n","\n","<center>\n","Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego \n","Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n","Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\" \n","Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n","    </center>"]},{"cell_type":"markdown","metadata":{"id":"ziZ9i7tXbO1T"},"source":["In this lab, you will implement some of the techniques discussed in the lecture.\n","\n","Below you are given a solution to the previous scenario. Note that it has two serious drawbacks:\n"," * The output predictions do not sum up to one (i.e. it does not return a distribution) even though the images always contain exactly one digit.\n"," * It uses MSE coupled with output sigmoid which can lead to saturation and slow convergence \n","\n","**Task 0.** Implement a numerically stable version of softmax.\n","\n","**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two functions as a single block and not even compute the gradient over the softmax values. \n","\n","**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n","\n","**Task 3 (optional).** Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts, etc.). Again, test to see how these changes improve accuracy/convergence.\n","\n","**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n","\n","The provided model evaluation code (`evaluate_model`) may take some time to complete. During implementation, you can change the number of evaluated models to 1 and reduce the number of tested learning rates, and epochs.  \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install tqdm\n","!pip install pandas"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"P22HqX9AbO1a"},"outputs":[],"source":["import random\n","import numpy as np\n","from torchvision import datasets, transforms\n","from typing import List, Any, Tuple, Optional, Callable, Dict\n","from numpy.typing import NDArray\n","import json\n","from tqdm import tqdm\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467,"referenced_widgets":["0887eb284cde433a93dc63af32cc9d9b","358d8edb5961494884a519823853bbc1","7d1eed65ecca478498920df1c94a7ab1","4cc42c27336c4004b8636f12fc602c83","8e6fd5210a7348658773944836baa32d","a411ff8732a84c4ca7c4619a848c773a","f78d21be3caf4c00b6d38d6a9f41acd0","3644dba634e84dbcba3cefcfe8491a05","5410d60a73fb4378bfcde8a43a5f75f5","5c9484a4d1b84db0aa4b37f215af9857","05a12b77a0344f168b0ce86235411193","3c475797b8c049bc900bdb82c9cee039","0eb41ae3f7c344728e7610e0ac2e85a0","5422f00593564627b8b1dbfe5c35375e","1399a04e36fb44a0adb69561d892702b","77df2c4f36ba43768f4b1afd46476f40","b1ef11936cde4ac8ac4cb6fd24c75ede","f676a65904b44a9baefab8c2a60e0eac","6df1299ec9ef42098b23a7bd33291730","36d9aac0f6324cc38e241ececa6a4983","c34a23042e964b4395cc60915133b2ba","fcda01da20f04bc0a9105f8b2baf9f2a","179b581ba55d4666a7b598aa16d379c1","d89d4753c54a4106853af165e3580381","73d1c89e46e346978a24dfbdb3d107f9","001c0f369ea04cdc87329ba70bc1da41","6bc73c9a0f414c14ba1ba549122c6c9b","937845c378944bcfba102e57219117fc","a0ff7bcdb6f244d3945dea309ffdef09","457f5c77ef924c8d939c36fbdb525c3c","5ab3ead1b8ee4a62b0e732e88a4b612b","6a6f8922ea26440ab907305b2f73c184","3aded8ec6c8c435db31880259a9c1d6f","7d6ec88d5f9b4b21b686f09fa415bd91","2b08ddda39424469a3248b0f152b9248","c5ef09e329c646ca98aebd21525f060e","1b9c5bf3d8ae457a96d50baf49906929","df3341a327524a1691aa7aca0006c490","23a28a753e7c4234b064d55ba0d35eec","e170ca7391a946b6a9febe117c16d87c","93588a37aca74159889b81666166d80a","12c9382de5be4afa97c32ad53880f443","095b39beff514e799541c63dabc90507","5a5c2ab34a0c4661b9c0611200e70e2b"]},"executionInfo":{"elapsed":1299,"status":"ok","timestamp":1635823776348,"user":{"displayName":"Marcin Mucha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlkem9VmPW7wSPwLDYxT54wMMY6iF7Evvmx95duw=s64","userId":"08004029552514744907"},"user_tz":-60},"id":"N9jGPaZhbO2B","outputId":"aec94d43-a23e-474a-d748-e756ba658ca3"},"outputs":[],"source":["# Let's read the mnist dataset\n","\n","\n","def load_mnist(path: str = \".\"):\n","    train_set = datasets.MNIST(path, train=True, download=True)\n","    x_train = train_set.data.numpy()\n","    _y_train = train_set.targets.numpy()\n","\n","    test_set = datasets.MNIST(path, train=False, download=True)\n","    x_test = test_set.data.numpy()\n","    _y_test = test_set.targets.numpy()\n","\n","    x_train = x_train.reshape((x_train.shape[0], 28 * 28)) / 255.0\n","    x_test = x_test.reshape((x_test.shape[0], 28 * 28)) / 255.0\n","\n","    y_train = np.zeros((_y_train.shape[0], 10))\n","    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n","\n","    y_test = np.zeros((_y_test.shape[0], 10))\n","    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n","\n","    return (x_train, y_train), (x_test, y_test)\n","\n","\n","(x_train, y_train), (x_test, y_test) = load_mnist()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"w3gAyqw4bO1p"},"outputs":[],"source":["def sigmoid(z: NDArray[float]):\n","    return 1.0 / (1.0 + np.exp(-z))\n","\n","\n","def sigmoid_prime(z: NDArray[float]):\n","    # Derivative of the sigmoid\n","    return sigmoid(z) * (1 - sigmoid(z))"]},{"cell_type":"markdown","metadata":{},"source":["## Warm-Up\n","Implement a numerically stable version of softmax.  \n","In general, softmax is defined as  \n","$$\\text{softmax}(x_1, x_2, \\ldots, x_n) = (\\frac{e^{x_1}}{\\sum_i{e^{x_i}}}, \\frac{e^{x_2}}{\\sum_i{e^{x_i}}}, \\ldots, \\frac{e^{x_n}}{\\sum_i{e^{x_i}}})$$  \n","However, taking $e^{1000000}$ can result in NaN.  \n","Can you implement softmax so that the highest power to which e will be risen will be at most $0$ and the predictions will be mathematically equivalent?  \n","\n","Hint: <sub><sub><sub>sǝnlɐʌ llɐ ɯoɹɟ ʇᴉ ʇɔɐɹʇqns  puɐ ᴉ‾x ʇsǝƃɹɐl ǝɥʇ ǝʞɐʇ</sub></sub></sub>"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def unstable_softmax(x: NDArray[float], axis: int = -1):\n","    e = np.exp(x)\n","    return e / np.sum(e, axis=axis, keepdims=True)\n","\n","\n","def stable_softmax(x: NDArray[float], axis: int = -1):\n","    ## TODO\n","    ###{\n","    pass\n","    ###}\n","\n","\n","### TESTS ###\n","def test_one(x: NDArray[float], y: NDArray[float], proc_fn: Callable[[NDArray[float]], NDArray[float]]):\n","    r = stable_softmax(x)\n","    assert r.shape == x.shape\n","    r = proc_fn(r)\n","    assert r.shape == y.shape\n","    diff = np.mean(np.abs(r - y))\n","    assert diff <= 1e-5, r\n","\n","\n","x1 = np.random.rand(100, 32).astype(np.float64)\n","test_one(x1, np.ones(100), lambda x: x.sum(-1))\n","test_one(x1, unstable_softmax(x1), lambda x: x)\n","\n","\n","x2 = np.ones((100, 32), dtype=np.float64) * 1e6\n","test_one(x2, np.ones_like(x2) / x2.shape[-1], lambda x: x)\n","### TESTS END ###"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FgEA2XRRbO2X","outputId":"f1672e48-5e18-4ad2-aa13-98e4e07a5fdc"},"outputs":[],"source":["class Network(object):\n","    def __init__(self, sizes: List[int]):\n","        # initialize biases and weights with random normal distr.\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y) for y in sizes[1:]]\n","        self.weights = [np.random.randn(x, y) for x, y in zip(sizes[:-1], sizes[1:])]\n","\n","    def feedforward(self, a: NDArray[float]) -> NDArray[float]:\n","        # Run the network on a single case\n","        for b, w in zip(self.biases, self.weights):\n","            a = sigmoid(a @ w + b)\n","\n","        return a\n","\n","    def update_mini_batch(\n","        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n","    ) -> None:\n","        # Update network weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","\n","        nabla_b, nabla_w = self.backprop(x_mini_batch, y_mini_batch)\n","\n","        self.weights = [w - eta * nw for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b - eta * nb for b, nb in zip(self.biases, nabla_b)]\n","\n","    def backprop(\n","        self, x: NDArray[float], y: NDArray[float]\n","    ) -> Tuple[List[NDArray[float]], List[NDArray[float]]]:\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","\n","        assert len(x.shape) == 2  # batch, features\n","        assert len(y.shape) == 2  # batch, classes\n","        assert x.shape[0] == y.shape[0]\n","\n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = []\n","        delta_nabla_w = []\n","\n","        # Then go forward remembering each layer input and value\n","        # before sigmoid activation\n","        layer_input = []\n","        before_act = []\n","        for w, b in zip(self.weights, self.biases):\n","            layer_input.append(x)\n","            x = x @ w + b\n","            before_act.append(x)\n","            x = sigmoid(x)\n","\n","        # Now go backward from the final cost applying backpropagation\n","        diff = self.cost_derivative(output_activations=x, y=y)\n","        for linp, bef_act, w, b in reversed(\n","            list(zip(layer_input, before_act, self.weights, self.biases))\n","        ):\n","            diff = sigmoid_prime(bef_act) * diff\n","            delta_nabla_w.append(linp.T @ diff)\n","            delta_nabla_b.append(np.sum(diff, axis=0))\n","            diff = diff @ w.T\n","\n","        delta_nabla_w = reversed(delta_nabla_w)\n","        delta_nabla_b = reversed(delta_nabla_b)\n","\n","        # Check shapes\n","        delta_nabla_b = list(delta_nabla_b)\n","        delta_nabla_w = list(delta_nabla_w)\n","        assert len(delta_nabla_b) == len(self.biases), (\n","            len(delta_nabla_b),\n","            len(self.biases),\n","        )\n","        assert len(delta_nabla_w) == len(self.weights), (\n","            len(delta_nabla_w),\n","            len(self.weights),\n","        )\n","        for lid in range(len(self.weights)):\n","            assert delta_nabla_b[lid].shape == self.biases[lid].shape, (\n","                delta_nabla_b[lid].shape,\n","                self.biases[lid].shape,\n","            )\n","            assert delta_nabla_w[lid].shape == self.weights[lid].shape, (\n","                delta_nabla_w[lid].shape,\n","                self.weights[lid].shape,\n","            )\n","\n","        return delta_nabla_b, delta_nabla_w\n","\n","    def evaluate(\n","        self, x_test_data: NDArray[float], y_test_data: NDArray[float]\n","    ) -> float:\n","        # Count the number of correct answers for test_data\n","        test_results = [\n","            (\n","                np.argmax(self.feedforward(x_test_data[i].reshape(1, 784)), axis=-1),\n","                np.argmax(y_test_data[i], axis=-1),\n","            )\n","            for i in range(len(x_test_data))\n","        ]\n","        # return accuracy\n","        return np.mean([int((x == y).item()) for (x, y) in test_results]).item()\n","\n","    def cost_derivative(\n","        self, output_activations: NDArray[float], y: NDArray[float]\n","    ) -> NDArray[float]:\n","        assert output_activations.shape == y.shape, (output_activations.shape, y.shape)\n","        return (output_activations - y) / len(y)  # moved here from update_mini_batch\n","\n","    def optimize(\n","        self,\n","        training_data: Tuple[NDArray[float], NDArray[float]],\n","        epochs: int,\n","        mini_batch_size: int,\n","        eta: float,\n","        test_data: Optional[Tuple[NDArray[float], NDArray[float]]] = None,\n","    ) -> None:\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        epoch_bar = tqdm(range(epochs), desc=\"Epoch\")\n","        for j in epoch_bar:\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[\n","                    i * mini_batch_size : (i * mini_batch_size + mini_batch_size)\n","                ]\n","                y_mini_batch = y_train[\n","                    i * mini_batch_size : (i * mini_batch_size + mini_batch_size)\n","                ]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                epoch_bar.set_description_str(\n","                    \"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test))\n","                )\n","            else:\n","                epoch_bar.set_description_str(\"Epoch: {0}\".format(j))\n","\n","        return self.evaluate(x_test, y_test)\n","\n","\n","def evaluate_model(\n","    model_name: str,\n","    model_constructor: Callable[[List[int]], Network],\n","    result_storage: Dict[str, List[Any]],\n","    layers: List[int] = [784, 30, 10],\n","):\n","    # Remove logs from the previous evaluation of the same model\n","    result_storage[\"MODEL\"] = [\n","        m for m in result_storage.get(\"MODEL\", []) if m != model_name\n","    ]\n","    result_storage[\"LR\"] = [\n","        lr\n","        for m, lr in zip(result_storage.get(\"MODEL\", []), result_storage.get(\"LR\", []))\n","        if m != model_name\n","    ]\n","    result_storage[\"ACC_STD\"] = [\n","        acc_std\n","        for m, acc_std in zip(\n","            result_storage.get(\"MODEL\", []), result_storage.get(\"ACC_STD\", [])\n","        )\n","        if m != model_name\n","    ]\n","    result_storage[\"ACC\"] = [\n","        acc\n","        for m, acc in zip(\n","            result_storage.get(\"MODEL\", []), result_storage.get(\"ACC\", [])\n","        )\n","        if m != model_name\n","    ]\n","\n","    for lr in [0.001, 0.01, 0.1, 1.0, 10.0]:\n","\n","        print(f\"Checking with lr = {lr}\")\n","        np.random.seed(42)\n","        accuracy_list = []\n","        for i in range(3):\n","            network = model_constructor(layers)\n","            accuracy = network.optimize(\n","                (x_train, y_train),\n","                epochs=10,\n","                mini_batch_size=100,\n","                eta=lr,\n","                test_data=(x_test, y_test),\n","            )\n","            accuracy_list.append(accuracy)\n","\n","        result_storage[\"MODEL\"].append(model_name)\n","        result_storage[\"LR\"].append(lr)\n","        result_storage[\"ACC_STD\"].append(\n","            f\"{np.mean(accuracy_list) * 100:2.1f}% +- {np.std(accuracy_list) * 100:.1f}\"\n","        )\n","\n","        result_storage[\"ACC\"].append(np.mean(accuracy_list))\n","\n","    df = pd.DataFrame(result_storage).sort_values(\"ACC\", ascending=False)\n","    df = df[[c for c in df.columns if c != \"ACC\"]]\n","    return df\n","\n","\n","RESULTS = {}\n","evaluate_model(\n","    model_name=\"Base\",\n","    model_constructor=lambda x: Network(x),\n","    result_storage=RESULTS,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Task 1 \n","Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence.   \n","Hint: When implementing backprop it might be easier to consider these two functions as a single block and not even compute the gradient over the softmax values.  \n","If you have problems, please see Appendix A.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Task1(Network):\n","    def __init__(self, sizes: List[int]):\n","        # initialize biases and weights with random normal distr.\n","        super().__init__(sizes=sizes)\n","\n","    def feedforward(self, a: NDArray[float]) -> NDArray[float]:\n","        # Run the network on a single case\n","\n","        ## TODO\n","        ###{\n","        pass\n","        ###}\n","\n","        return a\n","\n","    def backprop(\n","        self, x: NDArray[float], y: NDArray[float]\n","    ) -> Tuple[List[NDArray[float]], List[NDArray[float]]]:\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","\n","        assert len(x.shape) == 2  # batch, features\n","        assert len(y.shape) == 2  # batch, classes\n","        assert x.shape[0] == y.shape[0]\n","\n","        ##TODO\n","        ###{\n","        pass\n","        ###}\n","\n","        # Check shapes\n","        delta_nabla_b = list(delta_nabla_b)\n","        delta_nabla_w = list(delta_nabla_w)\n","        assert len(delta_nabla_b) == len(self.biases), (\n","            len(delta_nabla_b),\n","            len(self.biases),\n","        )\n","        assert len(delta_nabla_w) == len(self.weights), (\n","            len(delta_nabla_w),\n","            len(self.weights),\n","        )\n","        for lid in range(len(self.weights)):\n","            assert delta_nabla_b[lid].shape == self.biases[lid].shape, (\n","                delta_nabla_b[lid].shape,\n","                self.biases[lid].shape,\n","            )\n","            assert delta_nabla_w[lid].shape == self.weights[lid].shape, (\n","                delta_nabla_w[lid].shape,\n","                self.weights[lid].shape,\n","            )\n","\n","        return delta_nabla_b, delta_nabla_w\n","\n","    def cost_derivative(\n","        self, output_activations: NDArray[float], y: NDArray[float]\n","    ) -> NDArray[float]:\n","        assert output_activations.shape == y.shape, (output_activations.shape, y.shape)\n","        ## TODO\n","        ###{\n","        pass\n","        ###}\n","\n","\n","evaluate_model(\n","    model_name=\"SoftMax\",\n","    model_constructor=lambda x: Task1(x),\n","    result_storage=RESULTS,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Task 2\n","Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.  \n","A few notes:\n","* do not regularize the biases\n","* you can see an example pseudocode here [pytorch.org/docs/stable/generated/torch.optim.SGD.html](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Task2(Network):\n","    def __init__(\n","        self, sizes: List[int], l2_factor: float = 1e-4, momentum: float = 0.05\n","    ):\n","        # initialize biases and weights with random normal distr.\n","        super().__init__(sizes=sizes)\n","        self.l2_factor = l2_factor\n","        self.momentum = momentum\n","        ## TODO\n","        ####{\n","        pass\n","        ###}\n","\n","    def update_mini_batch(\n","        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n","    ) -> None:\n","        # Update network weights and biases by applying a single step\n","        # of gradient descent (with momentum and l2 regularization) using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        ## TODO\n","        ###{\n","        pass\n","        ###}\n","\n","\n","evaluate_model(\n","    model_name=\"L2&Momentum\",\n","    model_constructor=lambda x: Task2(x),\n","    result_storage=RESULTS,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Task 3 (optional)\n","Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts etc.). Again, test to see how these changes improve accuracy/convergence.  \n","In case you want to learn about AdamW you can check the official [PyTorch docummentation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) that contains pseudocode and the original paper [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).  \n","In Appendix B you can find a simplified version of AdaGrad.  \n","Below you can find brief information regarding the dropout:  \n","\n","During the training phase, we want to make some activations $0$.\n","It is usually implemented by zeroing each activation in the considered layer with probability $p$, and multiplying other activations by $\\frac{1}{1-p}$."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Task3Optimizer(Network):\n","    def __init__(\n","        self,\n","        sizes: List[int],\n","        weight_decay: float = 0.01,\n","        beta_1: float = 0.9,\n","        beta_2: float = 0.95,\n","        eps=1e-5,\n","    ):\n","        # initialize biases and weights with random normal distr.\n","        super().__init__(sizes=sizes)\n","        self.weight_decay = weight_decay\n","        self.beta_1 = beta_1\n","        self.beta_2 = beta_2\n","        self.eps = eps\n","        ## TODO\n","        ####{\n","        pass\n","        ###}\n","\n","    def update_mini_batch(\n","        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n","    ) -> None:\n","        # Update network weights and biases by applying a single step\n","        # of gradient descent (with momentum and weight decay) using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        ## TODO\n","        ###{\n","        pass\n","        ###}\n","\n","\n","evaluate_model(\n","    model_name=\"Optimizer\",\n","    model_constructor=lambda x: Task3Optimizer(x),\n","    result_storage=RESULTS,\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Place for remaining parts of task 3"]},{"cell_type":"markdown","metadata":{},"source":["## Task 4\n","Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]  \n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["## TODO"]},{"cell_type":"markdown","metadata":{},"source":["# Appendix"]},{"cell_type":"markdown","metadata":{},"source":["## Appendix A - Log-loss and softmax\n","\n","Let's compute the following derivative\n","$$\\frac{\\partial\\left(\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}\\right)}{\\partial z_{b, f}}$$\n","\n","\n","where\n","$$\n","y_{b, i} = \\begin{cases}\n","1 & \\text{if $b$'th image belongs to $i$'th class}\\\\\n","0 & \\text{otherwise}\n","\\end{cases}\n","$$\n","\n","\n","To do so we can use the chain rule:\n","$$\n","\\frac{\\partial f(g(x))}{\\partial x} = \\frac{\\partial f(g(x))}{\\partial g(x)}  \\frac{\\partial g(x)}{\\partial x}\n","$$\n","the rule for the sum  \n","$$\n","\\frac{\\partial (f(x) + g(x))}{\\partial x} = \\frac{\\partial f(x)}{\\partial x} +   \\frac{\\partial g(x)}{\\partial x}\n","$$\n","and the fact that the derivative of natural logarithm is\n","$$\n","\\frac{\\partial \\log(x)}{\\partial x} = \\frac{1}{x}\n","$$\n","\n","Using those rules we can observe that indeed:\n","\n","$$\n","\\frac{\\partial  \\left(\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}\\right)}{\\partial z_{b, f}} = \\sum_{i}{y_{b,i} \\frac{1}{\\text{prediction}_{b, i}} \\frac{ \\partial \\text{prediction}_{b, i}}{\\partial z_{b, f}}}\n","$$\n","\n","\n","\n","Let us review the multiplication rule:\n","$$\n","\\frac{\\partial (f(x)g(x))}{\\partial x} = \\frac{\\partial f(x)}{\\partial x}g(x)  + f(x)\\frac{\\partial g(x)}{\\partial x}\n","$$\n","and that\n","$$\n","\\frac{\\partial e^x}{\\partial x} = e^x\n","$$\n","\n","From the definition:\n","$$\n","\\text{prediction}_{b, f} = \\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\n","$$\n","\n","\n","We can observe that for $k \\not= f$\n","$$\n","\\frac{ \\partial \\text{prediction}_{b, k}}{\\partial z_{b, f}}\n","= 0 - \\frac{e^{z_{k, b}} e^{z_{b, f}} }{\\left(\\sum_{i}{e^{z_{b, i}}}\\right)^2}\n","$$\n","and that\n","$$\n","\\frac{ \\partial \\text{prediction}_{b, f}}{\\partial z_{b, f}}\n","= \\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}} - \\left(\\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\\right)^2\n","$$\n","\n","Therefore \n","$$\n","y_{b,i} \\frac{1}{\\text{prediction}_{b, i}} \\frac{ \\partial \\text{prediction}_{b, i}}{\\partial z_{b, f}} = \\begin{cases}\n","y_{b,i}\\left(-\\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\\right) & \\text{if } i \\not= f\\\\\n","y_{b,i}\\left(1 - \\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\\right) & \\text{if } i = f\\\\\n","\\end{cases}\n","$$\n","in other words\n","$$\n","y_{b,i} \\frac{1}{\\text{prediction}_{b, i}} \\frac{ \\partial \\text{prediction}_{b, i}}{\\partial z_{b, f}} = \\begin{cases}\n","y_{b,i}\\left(-\\text{prediction}_{b, f}\\right) & \\text{if } i \\not= f\\\\\n","y_{b,i}\\left(1 - \\text{prediction}_{b, f}\\right) & \\text{if } i = f\\\\\n","\\end{cases}\n","$$\n","\n","As we just sum over $i$, and for each $b$ there is exactly one $i$ such that $y_{b, i} = 1$ and for $j \\not= i$ $y_{b, j} = 0$, therefore  in the end we have that \n","$$\n","\\frac{\\partial  \\left(\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}\\right)}{\\partial z_{b, f}} = y_{b, f} - \\text{prediction}_{b, f}\n","$$\n","what can be written as:\n","$$\n","y - prediction\n","$$\n","\n","In the code, we are going to use \n","$$\n","prediction - y\n","$$\n","as our loss is\n","$$-\\sum_{b}\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}$$"]},{"cell_type":"markdown","metadata":{},"source":["## Appendix B - Adagrad (simplified version)\n","\n","Let $p_1, \\ldots, p_n$ be all parameters in our model (weights and biases).  \n","For parameter $p_i$ we maintain an array $G_i$ (can be set to $0$ initially).\n","Let $\\text{LOSS}$ be our loss without L2.   \n","We update $G_i$ and $p_i$ each training step as follows:  \n","$$\n","G_i = G_i +  \\left(\\frac{\\partial \\text{LOSS}}{\\partial p_i}\\right)^2\\\\\n","p_i = p_i - \\frac{\\eta}{\\sqrt{\\left(G_i + \\epsilon\\right)}}\\frac{\\partial \\text{LOSS}}{\\partial p_i}\n","$$"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1mbRybTuEd5hfMgPq47jAy40aizNX03x8","timestamp":1665425459792},{"file_id":"1hs2ViNkY7vFE7l_PL7b-2XIN17cxbsyL","timestamp":1635823858600},{"file_id":"1t76la2tUWVLnEK7IKxdFZn_Y0be3xZud","timestamp":1635823610862}]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"001c0f369ea04cdc87329ba70bc1da41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ab3ead1b8ee4a62b0e732e88a4b612b","max":1648877,"min":0,"orientation":"horizontal","style":"IPY_MODEL_457f5c77ef924c8d939c36fbdb525c3c","value":1648877}},"05a12b77a0344f168b0ce86235411193":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0887eb284cde433a93dc63af32cc9d9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d1eed65ecca478498920df1c94a7ab1","IPY_MODEL_4cc42c27336c4004b8636f12fc602c83","IPY_MODEL_8e6fd5210a7348658773944836baa32d"],"layout":"IPY_MODEL_358d8edb5961494884a519823853bbc1"}},"095b39beff514e799541c63dabc90507":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0eb41ae3f7c344728e7610e0ac2e85a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12c9382de5be4afa97c32ad53880f443":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1399a04e36fb44a0adb69561d892702b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_36d9aac0f6324cc38e241ececa6a4983","max":28881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6df1299ec9ef42098b23a7bd33291730","value":28881}},"179b581ba55d4666a7b598aa16d379c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73d1c89e46e346978a24dfbdb3d107f9","IPY_MODEL_001c0f369ea04cdc87329ba70bc1da41","IPY_MODEL_6bc73c9a0f414c14ba1ba549122c6c9b"],"layout":"IPY_MODEL_d89d4753c54a4106853af165e3580381"}},"1b9c5bf3d8ae457a96d50baf49906929":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_12c9382de5be4afa97c32ad53880f443","max":4542,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93588a37aca74159889b81666166d80a","value":4542}},"23a28a753e7c4234b064d55ba0d35eec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b08ddda39424469a3248b0f152b9248":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"358d8edb5961494884a519823853bbc1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3644dba634e84dbcba3cefcfe8491a05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36d9aac0f6324cc38e241ececa6a4983":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aded8ec6c8c435db31880259a9c1d6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c475797b8c049bc900bdb82c9cee039":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5422f00593564627b8b1dbfe5c35375e","IPY_MODEL_1399a04e36fb44a0adb69561d892702b","IPY_MODEL_77df2c4f36ba43768f4b1afd46476f40"],"layout":"IPY_MODEL_0eb41ae3f7c344728e7610e0ac2e85a0"}},"457f5c77ef924c8d939c36fbdb525c3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4cc42c27336c4004b8636f12fc602c83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5410d60a73fb4378bfcde8a43a5f75f5","max":9912422,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3644dba634e84dbcba3cefcfe8491a05","value":9912422}},"5410d60a73fb4378bfcde8a43a5f75f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5422f00593564627b8b1dbfe5c35375e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f676a65904b44a9baefab8c2a60e0eac","placeholder":"​","style":"IPY_MODEL_b1ef11936cde4ac8ac4cb6fd24c75ede","value":""}},"5a5c2ab34a0c4661b9c0611200e70e2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ab3ead1b8ee4a62b0e732e88a4b612b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c9484a4d1b84db0aa4b37f215af9857":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a6f8922ea26440ab907305b2f73c184":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bc73c9a0f414c14ba1ba549122c6c9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3aded8ec6c8c435db31880259a9c1d6f","placeholder":"​","style":"IPY_MODEL_6a6f8922ea26440ab907305b2f73c184","value":" 1649664/? [00:00&lt;00:00, 20024693.20it/s]"}},"6df1299ec9ef42098b23a7bd33291730":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73d1c89e46e346978a24dfbdb3d107f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0ff7bcdb6f244d3945dea309ffdef09","placeholder":"​","style":"IPY_MODEL_937845c378944bcfba102e57219117fc","value":""}},"77df2c4f36ba43768f4b1afd46476f40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcda01da20f04bc0a9105f8b2baf9f2a","placeholder":"​","style":"IPY_MODEL_c34a23042e964b4395cc60915133b2ba","value":" 29696/? [00:00&lt;00:00, 648756.19it/s]"}},"7d1eed65ecca478498920df1c94a7ab1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f78d21be3caf4c00b6d38d6a9f41acd0","placeholder":"​","style":"IPY_MODEL_a411ff8732a84c4ca7c4619a848c773a","value":""}},"7d6ec88d5f9b4b21b686f09fa415bd91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c5ef09e329c646ca98aebd21525f060e","IPY_MODEL_1b9c5bf3d8ae457a96d50baf49906929","IPY_MODEL_df3341a327524a1691aa7aca0006c490"],"layout":"IPY_MODEL_2b08ddda39424469a3248b0f152b9248"}},"8e6fd5210a7348658773944836baa32d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05a12b77a0344f168b0ce86235411193","placeholder":"​","style":"IPY_MODEL_5c9484a4d1b84db0aa4b37f215af9857","value":" 9913344/? [00:00&lt;00:00, 25975870.14it/s]"}},"93588a37aca74159889b81666166d80a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"937845c378944bcfba102e57219117fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0ff7bcdb6f244d3945dea309ffdef09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a411ff8732a84c4ca7c4619a848c773a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1ef11936cde4ac8ac4cb6fd24c75ede":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c34a23042e964b4395cc60915133b2ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5ef09e329c646ca98aebd21525f060e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e170ca7391a946b6a9febe117c16d87c","placeholder":"​","style":"IPY_MODEL_23a28a753e7c4234b064d55ba0d35eec","value":""}},"d89d4753c54a4106853af165e3580381":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df3341a327524a1691aa7aca0006c490":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a5c2ab34a0c4661b9c0611200e70e2b","placeholder":"​","style":"IPY_MODEL_095b39beff514e799541c63dabc90507","value":" 5120/? [00:00&lt;00:00, 114088.88it/s]"}},"e170ca7391a946b6a9febe117c16d87c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f676a65904b44a9baefab8c2a60e0eac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f78d21be3caf4c00b6d38d6a9f41acd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcda01da20f04bc0a9105f8b2baf9f2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
