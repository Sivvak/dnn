{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxW4dJFDfX_a"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
    "\n",
    "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
    "<hr>\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
    "\n",
    "<center>\n",
    "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
    "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
    "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
    "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcTwzhX8fBqs"
   },
   "source": [
    "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "In this exercise, we are using high-level abstractions from torch.nn like nn.Linear.\n",
    "Note: during the next lab session we will go one level deeper and implement more things\n",
    "with bare hands.\n",
    "\n",
    "Tasks:\n",
    "\n",
    " 1. Read the code.\n",
    "\n",
    " 2. Check that the given implementation reaches 95% test accuracy for architecture input-128-128-10 after few epochs.\n",
    "\n",
    " 3. Add the option to use SGD with momentum instead of ADAM.\n",
    "\n",
    " 4. Experiment with different learning rates. Use the provided TrainingVisualizer\n",
    " to plot the learning curves and gradient-to-weight ratios. Compare visualizations\n",
    " for different learning rates for both ADAM and SGD with momentum.\n",
    "\n",
    " 5. Parameterize the constructor by a list of sizes of hidden layers of the MLP.\n",
    " Note that this requires creating a list of layers as an attribute of the Net class,\n",
    " and one can't use a standard Python list containing nn.Modules (why?).\n",
    " Check torch.nn.ModuleList.\n",
    "\n",
    "If you run this notebook locally then you may need to install some packages.\n",
    "It may be achieved by adding the following code cell to the notebook and running it:\n",
    "```\n",
    "!pip install torch torchvision plotly ipywidgets\n",
    "```\n",
    "This notebook can also utilize Colab GPU. However, remember to kill your GPU session after classes as otherwise, you may use all your free GPU time for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYAsziKffBFV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import plotly.graph_objects as go\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import output\n",
    "\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "ThUOOE4c8lAz"
   },
   "outputs": [],
   "source": [
    "# @title Visualize gradients\n",
    "\n",
    "\n",
    "class TrainingVisualizer:\n",
    "    def __init__(self, log_interval: int = 10):\n",
    "        self.log_interval = log_interval\n",
    "        self.train_loss_fig = self.init_line_plot(\n",
    "            title=\"Training loss\", xaxis_title=\"Step\"\n",
    "        )\n",
    "        self.grad_to_weight_fig = self.init_line_plot(\n",
    "            title=\"Gradient standard deviation to weight standard deviation ratio at 1st layer\",\n",
    "            xaxis_title=\"Step\",\n",
    "            yaxis_title=\"Gradient to weight ratio (log scale)\",\n",
    "            yaxis_type=\"log\",\n",
    "        )\n",
    "        self.test_acc_fig = self.init_line_plot(\n",
    "            title=\"Test accuracy\", x=[], xaxis_title=\"Epoch\", mode=\"lines+markers\"\n",
    "        )\n",
    "\n",
    "        # Parameters related to current tracked model and its training\n",
    "        self.first_linear_layer = None\n",
    "        self.lr = None\n",
    "        self.trace_idx = -1\n",
    "\n",
    "    def init_line_plot(\n",
    "        self,\n",
    "        title: str,\n",
    "        x=None,\n",
    "        xaxis_title: str = None,\n",
    "        yaxis_title: str = None,\n",
    "        yaxis_type: str = \"linear\",\n",
    "        mode: str = \"lines\",\n",
    "    ):\n",
    "        fig = go.Figure()\n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            title_x=0.5,\n",
    "            xaxis_title=xaxis_title,\n",
    "            yaxis_title=yaxis_title,\n",
    "            height=400,\n",
    "            width=1500,\n",
    "            margin=dict(b=10, t=60),\n",
    "        )\n",
    "        fig.update_yaxes(type=yaxis_type)\n",
    "        # We cannot add new traces dynamically because Colab has a problem with widgets\n",
    "        # from plotly (traces added dynamically are rendered twice).\n",
    "        # As an ugly workaround we create a lot of empty traces and update them later\n",
    "        # with actual data. Empty traces are not plotted.\n",
    "        for _ in range(25):\n",
    "            fig.add_trace(go.Scatter(x=x, y=[], showlegend=True, mode=mode))\n",
    "\n",
    "        fig_widget = go.FigureWidget(fig)\n",
    "        display(fig_widget)\n",
    "        return fig_widget\n",
    "\n",
    "    def track_model(\n",
    "        self, model: torch.nn.Module, optimizer: torch.optim.Optimizer, lr: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Start tracking training metrics for a new model.\n",
    "        \"\"\"\n",
    "\n",
    "        for field in model.__dict__[\"_modules\"].values():\n",
    "            if isinstance(field, nn.Linear):\n",
    "                self.first_linear_layer = field\n",
    "                break\n",
    "            elif isinstance(field, nn.ModuleList):\n",
    "                self.first_linear_layer = field[0]\n",
    "                break\n",
    "\n",
    "        self.lr = lr\n",
    "        self.trace_idx += 1\n",
    "\n",
    "        optim_name = type(optimizer).__name__\n",
    "        self.train_loss_fig.data[self.trace_idx].name = f\"{optim_name}, {lr}\"\n",
    "        self.grad_to_weight_fig.data[self.trace_idx].name = f\"{optim_name}, {lr}\"\n",
    "        self.test_acc_fig.data[self.trace_idx].name = f\"{optim_name}, {lr}\"\n",
    "\n",
    "    def plot_gradients_and_loss(self, batch_idx: int, loss: float):\n",
    "        if batch_idx % self.log_interval == 0:\n",
    "            self.train_loss_fig.data[self.trace_idx].y += (loss,)\n",
    "\n",
    "            layer = self.first_linear_layer\n",
    "            grad_to_weight_ratio = (\n",
    "                self.lr * layer.weight.grad.std() / layer.weight.std()\n",
    "            ).item()\n",
    "\n",
    "            self.grad_to_weight_fig.data[self.trace_idx].y += (grad_to_weight_ratio,)\n",
    "\n",
    "    def plot_accuracy(self, epoch: int, accuracy: float):\n",
    "        self.test_acc_fig.data[self.trace_idx].x += (epoch,)\n",
    "        self.test_acc_fig.data[self.trace_idx].y += (accuracy,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMtap4QCfBH8"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # After flattening an image of size 28x28 we have 784 inputs\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    log_interval: int,\n",
    "    visualizer: TrainingVisualizer,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    # Setting the model to train mode makes model.training True.\n",
    "    # This can alter the behavior of modules like Dropout.\n",
    "    # This also applies to sub-modules if they are properly set\n",
    "    # Consider print(model.fc2.training)\n",
    "    # and then\n",
    "    # model.eval() or model.training()\n",
    "    # print(model.fc2.training)\n",
    "    model.train()\n",
    "    assert model.training\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # PyTorch will return an error if the model and data are on different devices.\n",
    "        # The two most popular devices are cpu and cuda.\n",
    "        # You can also use .to(torch_tyoe) to change type.\n",
    "        # For example consider data = data.to(torch.float32)\n",
    "        # For more, see https://pytorch.org/docs/stable/tensors.html\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Let p be a parameter of our model.\n",
    "        # Initially p.grad should be None.\n",
    "        # After we call loss.backward()\n",
    "        # p.grad() should be populated with (d loss)/(d p)\n",
    "        # optimizer.step() applies the gradient and updates optimizer stats.\n",
    "        # Whereas optimizer.zero_grad() clears the p.grad attribute.\n",
    "        # If we do not call  optimizer.zero_grad() then\n",
    "        # we will accumulate gradients from previous\n",
    "        # calls of loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        visualizer.plot_gradients_and_loss(batch_idx, loss.item())\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    epoch: int,\n",
    "    visualizer: TrainingVisualizer,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    # This makes model.training False\n",
    "    # What alters the behaviour of (sub)modules like Dropout\n",
    "    model.eval()\n",
    "    assert not model.training\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    # This disables gradient calculation.\n",
    "    # By default torch starts tracking calculations (what uses memory)\n",
    "    # for later backpropagation when it comes across a variable\n",
    "    # with requires_grad set to True.\n",
    "    # torch.no_grad prevents this\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss += F.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # this sum up batch loss, however usually we prefer a mean reduction\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "                test_loss,\n",
    "                correct,\n",
    "                len(test_loader.dataset),\n",
    "                100.0 * correct / len(test_loader.dataset),\n",
    "            )\n",
    "        )\n",
    "    visualizer.plot_accuracy(epoch, 100.0 * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5GlMs1-fBKP"
   },
   "outputs": [],
   "source": [
    "# training uses more memory than test due to gradient computation\n",
    "# therefore we can set test_batch_size to be larger\n",
    "batch_size = 256\n",
    "test_batch_size = 1000\n",
    "epochs = 5\n",
    "lr = 1e-2\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "# to check whether a cuda device is available\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgfUP23AfBMd"
   },
   "outputs": [],
   "source": [
    "# for reproductibility\n",
    "torch.manual_seed(seed)\n",
    "# if we have cuda capable device (ex, nvidia GPU) then we use it\n",
    "# otherwise we use cpu, there are also other devices like mps,\n",
    "# but their support for operations may be limited\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {\"batch_size\": batch_size}\n",
    "test_kwargs = {\"batch_size\": test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0KPoUtsfBOs",
    "outputId": "4ee308b0-0aac-4d3c-f372-352f28970104"
   },
   "outputs": [],
   "source": [
    "# data loader preparation\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NtsESKX8XrT"
   },
   "outputs": [],
   "source": [
    "visualizer = TrainingVisualizer(log_interval=log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezvIQbgsfBRT",
    "outputId": "3f6621ef-0bad-46c6-bd8f-ac535db8e9af"
   },
   "outputs": [],
   "source": [
    "# Note that there is a difference between taking tensor to device and model to device.\n",
    "# Consider writing model.to(torch.device(\"cpu\")) and some_tesnor.to(torch.device(\"cpu\"))\n",
    "# What seems to be happening in-place?\n",
    "model = Net().to(device)\n",
    "# Look at the output of list(model.parameters()) as you add new parameters to the model\n",
    "# What happens when the parameters are inside the standard list instead of torch.nn.ModuleList?\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "visualizer.track_model(model, optimizer, lr)\n",
    "\n",
    "print_details = False  # change if you want additional output\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(\n",
    "        model,\n",
    "        device,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        epoch,\n",
    "        log_interval,\n",
    "        visualizer,\n",
    "        verbose=print_details,\n",
    "    )\n",
    "    test(model, device, test_loader, epoch, visualizer, verbose=print_details)\n",
    "\n",
    "# investigate the results, what is the shape of test accuracy curve? Try to explain it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
