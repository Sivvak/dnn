{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcTwzhX8fBqs"
   },
   "source": [
    "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "In this exercise, we are going to implement a [ResNet-like](https://arxiv.org/pdf/1512.03385.pdf) architecture for the image classification task.\n",
    "The model is trained on the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "    1. Implement residual connections in the missing places in the code.\n",
    "\n",
    "    2. Check that the given implementation reaches 97% test accuracy after a few epochs.\n",
    "\n",
    "    3. Check that when extending the residual blocks to 20 (having 40+ layers total), the model still trains well, i.e., achieves 97+% accuracy after three epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d386b26",
   "metadata": {},
   "source": [
    "Note: in this lab scenario we are using mypy for typing. You can disable easily by not running the cell below.\n",
    "Typing in python is not mandatory, but if the types are natural, it can lead to less debugging, especially\n",
    "that types can be checked statically without running the code (typically done even within IDE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2eeffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nb-mypy -qqq\n",
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IYAsziKffBFV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# There is no typing for torchvision yet.\n",
    "from torchvision import datasets, transforms  # type: ignore\n",
    "from torch.utils.data import DataLoader\n",
    "from typing_extensions import TypedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: implement forward function\n",
    "        y = self.conv_block_2(self.conv_block_1(x))\n",
    "        z = x + y\n",
    "        return nn.functional.relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.rc = nn.Sequential(\n",
    "            ResidualConnection(1, 16),\n",
    "            # TODO: verify that after increasing 3 to 19 still trains\n",
    "            *(ResidualConnection(16, 16) for _ in range(19)),\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            28 * 28 * 16, 10\n",
    "        )  # 28 * 28 * 16 is the size of flattened output of the last ResidualConnection\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.rc(x)\n",
    "        x = nn.Flatten(start_dim=1)(x)\n",
    "        x = self.fc(x)\n",
    "        output = nn.LogSoftmax(dim=1)(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DMtap4QCfBH8"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    epoch: int,\n",
    "    log_interval: int,\n",
    ") -> None:\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\")\n",
    "    for batch_idx, (data, target) in pbar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "def test(model: nn.Module, device: torch.device, test_loader: DataLoader) -> None:\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    test_set_size = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            test_set_size += data.shape[0]\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= test_set_size\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            test_set_size,\n",
    "            100.0 * correct / test_set_size,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K5GlMs1-fBKP"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "test_batch_size = 1000\n",
    "epochs = 3\n",
    "lr = 1e-2\n",
    "seed = 1\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WgfUP23AfBMd"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "DataloaderArgs = TypedDict(\n",
    "    \"DataloaderArgs\",\n",
    "    {\"batch_size\": int, \"shuffle\": bool, \"num_workers\": int, \"pin_memory\": bool},\n",
    "    total=False,\n",
    ")\n",
    "\n",
    "train_kwargs: DataloaderArgs = {\"batch_size\": batch_size}\n",
    "test_kwargs: DataloaderArgs = {\"batch_size\": test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs: DataloaderArgs = {\n",
    "        \"num_workers\": 1,\n",
    "        \"pin_memory\": True,\n",
    "        \"shuffle\": True,\n",
    "    }\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0KPoUtsfBOs",
    "outputId": "4ee308b0-0aac-4d3c-f372-352f28970104"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "train_loader = DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezvIQbgsfBRT",
    "outputId": "3f6621ef-0bad-46c6-bd8f-ac535db8e9af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 235/235 [02:25<00:00,  1.61it/s, loss=0.0684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0374, Accuracy: 8902/10000 (89%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 235/235 [02:25<00:00,  1.62it/s, loss=0.357] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.8623, Accuracy: 8641/10000 (86%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 235/235 [02:26<00:00,  1.60it/s, loss=0.0113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1524, Accuracy: 9572/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "    test(model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
